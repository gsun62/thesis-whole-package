{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    roc_auc_score, \n",
    "    brier_score_loss,\n",
    "    classification_report, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models on Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = pd.read_csv('data/processed_actions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE DATA FOR MODELING\n",
    "\n",
    "# Define base features\n",
    "base_features = [\n",
    "    'ts_minute', 'ts_end', 'play_pattern', 'possession_team', 'team', 'player', 'type',\n",
    "    'location_x', 'location_y', 'end_location_x', 'end_location_y',  \n",
    "    'body_part', 'outcome', 'team_score', 'opponent_score', \n",
    "    'distance_x', 'distance_y', 'goal_diff_start', 'goal_diff_end',\n",
    "    'distance_goal', 'angle_goal', \n",
    "    'distance_goal_end', 'angle_goal_end', 'under_pressure'\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "r = 3   # number of previous actions to include\n",
    "k = 10  # look ahead horizon for target columns\n",
    "\n",
    "# Build feature columns list\n",
    "feature_cols = []\n",
    "for feat in base_features:\n",
    "    if feat in actions.columns:\n",
    "        feature_cols.append(feat)\n",
    "    for i in range(1, r):\n",
    "        col = f\"{feat}_prev_{i}\"\n",
    "        if col in actions.columns:\n",
    "            feature_cols.append(col)\n",
    "\n",
    "# Define targets\n",
    "score_target = f'team_scores_in_{k}'\n",
    "concede_target = f'team_concedes_in_{k}'\n",
    "\n",
    "if score_target not in actions.columns or concede_target not in actions.columns:\n",
    "    raise ValueError(f\"Target columns for k={k} are missing.\")\n",
    "\n",
    "# Split data so that 2021,2022 are train, 2023 is test\n",
    "train_data = actions[actions['match_id'] < 3881483]\n",
    "test_data = actions[actions['match_id'] >= 3881483] \n",
    "\n",
    "print(f\"Train observations: {len(train_data)} ({len(train_data['match_id'].unique())} matches)\")\n",
    "print(f\"Test observations: {len(test_data)} ({len(test_data['match_id'].unique())} matches)\")\n",
    "\n",
    "# Prepare features\n",
    "X_train = train_data[feature_cols].copy()\n",
    "X_test = test_data[feature_cols].copy()\n",
    "\n",
    "# Prepare targets\n",
    "y_score_train = train_data[score_target].astype(int)\n",
    "y_score_test = test_data[score_target].astype(int)\n",
    "\n",
    "y_concede_train = train_data[concede_target].astype(int)\n",
    "y_concede_test = test_data[concede_target].astype(int)\n",
    "\n",
    "y_match_train = (train_data['match_outcome'] == 'win').astype(int)\n",
    "y_match_test = (test_data['match_outcome'] == 'win').astype(int)\n",
    "\n",
    "y_possession_scores_train = train_data['possession_scores'].astype(int)\n",
    "y_possession_scores_test = test_data['possession_scores'].astype(int)\n",
    "\n",
    "y_possession_concedes_train = train_data['possession_concedes'].astype(int)\n",
    "y_possession_concedes_test = test_data['possession_concedes'].astype(int)\n",
    "\n",
    "# Identify categorical vs. numeric columns\n",
    "categorical_cols = [col for col in feature_cols if (col == \"team\" or \"team_prev\" in col or 'possession_team' in col or\n",
    "                                                   \"player\" in col or \"type\" in col or \n",
    "                                                   \"body_part\" in col or \"outcome\" in col\n",
    "                                                   or 'play_pattern' in col)]\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Convert categorical columns to string\n",
    "for col in categorical_cols:\n",
    "    X_train[col] = X_train[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "    \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', 'passthrough', numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Selection\n",
    "\n",
    "# Function to run randomized search CV for a specific model\n",
    "def run_randomized_search(X, y, model, param_dist, cv_strategy, n_iter=10, scoring='neg_brier_score', n_jobs=-1, verbose=3):\n",
    "    \"\"\"\n",
    "    Run randomized search cross-validation for a specific model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature dataset\n",
    "    y : array-like\n",
    "        Target variable\n",
    "    model : estimator object\n",
    "        The model to optimize\n",
    "    param_dist : dict\n",
    "        Parameter distribution for randomized search\n",
    "    n_iter : int, default=100\n",
    "        Number of parameter settings sampled\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    scoring : str, default='balanced_accuracy'\n",
    "        Scoring metric\n",
    "    n_jobs : int, default=-1\n",
    "        Number of parallel jobs (-1 means using all processors)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    RandomizedSearchCV object\n",
    "    \"\"\"\n",
    "    # Set up randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv_strategy,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=42,\n",
    "        verbose=verbose,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search\n",
    "\n",
    "# Helper function to compute 3 metrics of interest\n",
    "def compute_metrics(y, y_pred_proba):\n",
    "    metrics_dict = {\n",
    "        'accuracy': accuracy_score(y, (y_pred_proba>=0.5).astype(int)),\n",
    "        'neg_brier_score': brier_score_loss(y, y_pred_proba),\n",
    "        'roc_auc': roc_auc_score(y, y_pred_proba)\n",
    "    }\n",
    "    print(metrics_dict)\n",
    "    return metrics_dict\n",
    "\n",
    "# Function to compare all models after randomized search\n",
    "def compare_models(results_dict, X_dict, y, cv_strategy, metrics=['accuracy', 'neg_brier_score', 'roc_auc']):\n",
    "    \"\"\"\n",
    "    Compare the best results from each model.\n",
    "    \"\"\"\n",
    "    comparison = {}\n",
    "    \n",
    "    for name, result in results_dict.items():\n",
    "        best_model = result.best_estimator_\n",
    "        X = X_dict[name]\n",
    "        \n",
    "        # Cross-validate with best parameters\n",
    "        y_pred_proba = np.zeros_like(y, dtype=float)\n",
    "        for train_idx, test_idx in cv_strategy.split(X, y):\n",
    "            # Handle both DataFrame and numpy array\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            else:\n",
    "                X_train, X_test = X[train_idx], X[test_idx]\n",
    "            \n",
    "            y_train = y[train_idx]\n",
    "            best_model.fit(X_train, y_train)\n",
    "            y_pred_proba[test_idx] = best_model.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics_dict = compute_metrics(y, y_pred_proba)\n",
    "        \n",
    "        comparison[name] = {\n",
    "            'CV Score': result.best_score_,\n",
    "            'Best Params': result.best_params_,\n",
    "            **{m: metrics_dict[m] for m in metrics},\n",
    "            'Std Test Score': np.std(result.cv_results_['mean_test_score'])\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(comparison).T\n",
    "\n",
    "# Main function to run the entire model selection process\n",
    "def run_model_selection(models, X, y, cv_strategy, scoring='neg_brier_score', n_iter=100, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Run model selection using randomized search CV for multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature dataset\n",
    "    y : array-like\n",
    "        Target variable\n",
    "    n_iter : int, default=100\n",
    "        Number of parameter settings sampled for each model\n",
    "    class_weight : dict or 'balanced', default=None\n",
    "        Class weights for models that support it\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing model results\n",
    "    pandas.DataFrame\n",
    "        Model comparison DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Check class imbalance\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    imbalance_ratio = max(counts) / min(counts)\n",
    "    print(f\"Class distribution: {dict(zip(unique, counts))}\")\n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    # Run randomized search for each model\n",
    "    results = {}\n",
    "    X_dict = {}\n",
    "    \n",
    "    for name, config in models.items():\n",
    "        print(f\"\\nRunning randomized search for {name}...\")\n",
    "        results[name] = run_randomized_search(\n",
    "            config['X'], y, \n",
    "            config['model'], \n",
    "            config['params'], \n",
    "            n_iter=n_iter, \n",
    "            n_jobs=n_jobs,\n",
    "            scoring=scoring,\n",
    "            cv_strategy=cv_strategy\n",
    "        )\n",
    "        X_dict[name] = config['X']\n",
    "    \n",
    "    # Compare models\n",
    "    comparison_df = compare_models(results, X_dict, y, cv_strategy=cv_strategy)\n",
    "    \n",
    "    # Print best model\n",
    "    best_model_name = comparison_df[scoring].idxmax()\n",
    "    print(f\"\\nBest model: {best_model_name} with {scoring}: {comparison_df.loc[best_model_name, scoring]:.4f}\")\n",
    "    \n",
    "    return results, comparison_df, X_dict\n",
    "\n",
    "def eval_best_model(results, comparison, scoring, X_dict, y, cv_strategy):\n",
    "    # Display comparison\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(comparison)\n",
    "\n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    comparison[scoring].sort_values().plot(kind='barh')\n",
    "    plt.title(f'Model Comparison - {scoring}')\n",
    "    plt.xlabel(scoring)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Get the best overall model\n",
    "    best_model_name = comparison[scoring].idxmax()\n",
    "    best_model = results[best_model_name]\n",
    "\n",
    "    # Detailed evaluation of the best model\n",
    "    print(f\"\\nDetailed evaluation of the best model ({best_model_name}):\")\n",
    "    best_estimator = best_model.best_estimator_\n",
    "\n",
    "    print('Best model parameters:')\n",
    "    print(comparison['Best Params'].iloc[0])\n",
    "\n",
    "    y_pred = cross_val_predict(best_estimator, \n",
    "                                X_dict[best_model_name], \n",
    "                                y, \n",
    "                                cv=cv_strategy)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up 5-fold CV\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run CV metrics on the 5 models\n",
    "metrics = {\n",
    "    'Action Score Metrics:': y_score_train,\n",
    "    'Action Concede Metrics:': y_concede_train,\n",
    "    'Possession Score Metrics:': y_possession_scores_train,\n",
    "    'Possession Concede Metrics:': y_possession_concedes_train,\n",
    "    'Match Outcome Metrics:': y_match_train,\n",
    "}\n",
    "\n",
    "# Loop over each metric, train the classifier, compute CV predictions, and report metrics.\n",
    "for label, y in metrics.items():\n",
    "    # Same settings for each\n",
    "    cb = CatBoostClassifier(\n",
    "        random_state=42,\n",
    "        verbose=1,\n",
    "        cat_features=categorical_cols,\n",
    "        iterations=200,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=3,\n",
    "        random_strength=1.56\n",
    "    )\n",
    "    y_pred_proba = cross_val_predict(\n",
    "        estimator=cb,\n",
    "        X=X_train,\n",
    "        y=y,\n",
    "        cv=cv_strategy,\n",
    "        method='predict_proba'\n",
    "    )[:, 1]\n",
    "    \n",
    "    print(label)\n",
    "    compute_metrics(y, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training and evaluation setup\n",
    "def simple_eval(X, y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]  # Probability of positive class\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    auc = roc_auc_score(y, y_pred_proba)\n",
    "    brier = brier_score_loss(y, y_pred_proba)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"AUC Score: {auc}\")\n",
    "    print(f\"Brier Score: {brier}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    return acc, auc, brier\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, model_class=CatBoostClassifier,\n",
    "                       model_params=None, model_name=\"Model\", feature_names=None):\n",
    "    \"\"\"Generic function to train and evaluate a model with AUC and Brier score\"\"\"\n",
    "    \n",
    "    # Training\n",
    "    model = model_class(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation\n",
    "    print(f'Train eval for {model_name}')\n",
    "    simple_eval(X_train, y_train, model)\n",
    "    print(f'Test eval for {model_name}')\n",
    "    test_acc, test_auc, test_brier = simple_eval(X_test, y_test, model)\n",
    "    \n",
    "    # Plot feature importance if feature names are provided\n",
    "    if feature_names is not None:\n",
    "        # Get CatBoost feature importances\n",
    "        feature_importances = model.get_feature_importance()\n",
    "        plot_feature_importances(feature_importances, feature_names, f'Feature Importances: {model_name}')\n",
    "    \n",
    "    return model, test_acc, test_auc, test_brier\n",
    "\n",
    "def plot_feature_importances(importances, feature_names, title, top_n=20):\n",
    "    # Convert feature_names to list if it's not already\n",
    "    feature_names = list(feature_names)\n",
    "    # Create a DataFrame for easier sorting\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance and get top n features\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    top_n = min(top_n, len(importance_df))\n",
    "    importance_df = importance_df.head(top_n).iloc[::-1]\n",
    "    \n",
    "    # Create a horizontal bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(np.arange(top_n), importance_df['importance'], align='center')\n",
    "    plt.yticks(np.arange(top_n), importance_df['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the training and evaluation\n",
    "\n",
    "# Define common feature names and parameters for all models\n",
    "feature_names = X_train.columns.tolist()\n",
    "catboost_params = {\n",
    "    'iterations': 200,\n",
    "    'depth': 8,\n",
    "    'random_state': 42,\n",
    "    'l2_leaf_reg': 3, \n",
    "    'random_strength':1.56,\n",
    "    'cat_features':categorical_cols\n",
    "}\n",
    "\n",
    "# Model 1: Team Scores\n",
    "catboost_score, acc_score, auc_score, brier_score = train_and_evaluate(\n",
    "    X_train, y_score_train,\n",
    "    X_test, y_score_test,\n",
    "    model_params=catboost_params,\n",
    "    model_name=f\"Team Scores in {k}\",\n",
    "    feature_names=feature_names \n",
    ")\n",
    "# Model 2: Team Concedes\n",
    "catboost_concede, acc_concede, auc_concede, brier_concede = train_and_evaluate(\n",
    "    X_train, y_concede_train,\n",
    "    X_test, y_concede_test,\n",
    "    model_params=catboost_params,\n",
    "    model_name=f\"Team Concedes in {k}\",\n",
    "    feature_names=feature_names \n",
    ")\n",
    "# Model 3: Match Outcome\n",
    "catboost_match, acc_match, auc_match, brier_match = train_and_evaluate(\n",
    "    X_train, y_match_train,\n",
    "    X_test, y_match_test,\n",
    "    model_params=catboost_params,\n",
    "    model_name=\"Match Outcome\",\n",
    "    feature_names=feature_names \n",
    ")\n",
    "# Model 4: Possession Scores\n",
    "catboost_poss_score, acc_poss_score, auc_poss_score, brier_poss_score = train_and_evaluate(\n",
    "    X_train, y_possession_scores_train,\n",
    "    X_test, y_possession_scores_test,\n",
    "    model_params=catboost_params,\n",
    "    model_name=\"Possession Scores\",\n",
    "    feature_names=feature_names \n",
    ")\n",
    "# Model 5: Possession Concedes\n",
    "catboost_poss_concede, acc_poss_concede, auc_poss_concede, brier_poss_concede = train_and_evaluate(\n",
    "    X_train, y_possession_concedes_train,\n",
    "    X_test, y_possession_concedes_test,\n",
    "    model_params=catboost_params,\n",
    "    model_name=\"Possession Concedes\",\n",
    "    feature_names=feature_names \n",
    ")\n",
    "\n",
    "# Collect results in a DataFrame for easy comparison\n",
    "results = pd.DataFrame({\n",
    "    'Model': [f\"Team Scores in {k}\", f\"Team Concedes in {k}\", \"Match Outcome\", \n",
    "              \"Possession Scores\", \"Possession Concedes\"],\n",
    "    'Accuracy': [acc_score, acc_concede, acc_match, acc_poss_score, acc_poss_concede],\n",
    "    'AUC Score': [auc_score, auc_concede, auc_match, auc_poss_score, auc_poss_concede],\n",
    "    'Brier Score': [brier_score, brier_concede, brier_match, brier_poss_score, brier_poss_concede]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_probabilities(data, X_processed, models, prob_column_names):\n",
    "    \"\"\"\n",
    "    Add prediction probabilities from various models to a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        The dataset to add probabilities to\n",
    "    X_processed : numpy.ndarray or scipy.sparse.matrix\n",
    "        Processed features\n",
    "    models : list\n",
    "        List of trained classifier models with predict_proba method\n",
    "    prob_column_names : list\n",
    "        List of column names for the probability outputs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Original dataframe with probability columns added\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result = data.copy()\n",
    "    \n",
    "    # For each model and column name, add the positive class probability\n",
    "    for model, col_name in zip(models, prob_column_names):\n",
    "        result[col_name] = model.predict_proba(X_processed)[:, 1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Specify models\n",
    "models = [catboost_score, catboost_concede, catboost_match, catboost_poss_score, catboost_poss_concede]\n",
    "prob_columns = ['score_prob', 'concede_prob', 'win_prob', 'poss_score_prob', 'poss_concede_prob']\n",
    "\n",
    "# Add probabilities to training and test data\n",
    "train_data = add_prediction_probabilities(\n",
    "    train_data, X_train, models, prob_columns\n",
    ")\n",
    "test_data = add_prediction_probabilities(\n",
    "    test_data, X_test, models, prob_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(data):\n",
    "    \"\"\"\n",
    "    Compute VAEP, WPA, and GPA metrics for a given dataset, accounting for team changes within each match.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing 'match_id', 'team', 'score_prob', 'concede_prob', \n",
    "                           'win_prob', 'poss_score_prob', and 'poss_concede_prob' columns.\n",
    "                           Assumes data is sorted by 'match_id' and action sequence within each match.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added 'VAEP', 'WPA', and 'GPA' columns.\n",
    "    \"\"\"\n",
    "    # Shift probabilities and team within each match_id to get previous action data\n",
    "    data['prev_score_prob'] = data.groupby('match_id')['score_prob'].shift()\n",
    "    data['prev_concede_prob'] = data.groupby('match_id')['concede_prob'].shift()\n",
    "    data['prev_win_prob'] = data.groupby('match_id')['win_prob'].shift()\n",
    "    data['prev_poss_score_prob'] = data.groupby('match_id')['poss_score_prob'].shift()\n",
    "    data['prev_poss_concede_prob'] = data.groupby('match_id')['poss_concede_prob'].shift()\n",
    "    data['prev_team'] = data.groupby('match_id')['team'].shift()\n",
    "    \n",
    "    # Calculate team_change: 1 if the team changed from the previous action or if it's the first action in the match\n",
    "    data['team_change'] = (data['team'] != data['prev_team']).astype(int)\n",
    "    \n",
    "    # Adjust previous probabilities based on team change\n",
    "    # If team_change == 0 (same team), use the previous probabilities as is\n",
    "    # If team_change == 1 (team changed or first action), swap score and concede probabilities\n",
    "    data['adjusted_prev_score_prob'] = np.where(\n",
    "        data['team_change'] == 0,\n",
    "        data['prev_score_prob'],\n",
    "        data['prev_concede_prob']\n",
    "    )\n",
    "    data['adjusted_prev_concede_prob'] = np.where(\n",
    "        data['team_change'] == 0,\n",
    "        data['prev_concede_prob'],\n",
    "        data['prev_score_prob']\n",
    "    )\n",
    "    data['adjusted_prev_win_prob'] = np.where(\n",
    "        data['team_change'] == 0,\n",
    "        data['prev_win_prob'],\n",
    "        1 - data['prev_win_prob']  # For win probability, invert it when team changes\n",
    "    )\n",
    "    data['adjusted_prev_poss_score_prob'] = np.where(\n",
    "        data['team_change'] == 0,\n",
    "        data['prev_poss_score_prob'],\n",
    "        data['prev_poss_concede_prob']\n",
    "    )\n",
    "    data['adjusted_prev_poss_concede_prob'] = np.where(\n",
    "        data['team_change'] == 0,\n",
    "        data['prev_poss_concede_prob'],\n",
    "        data['prev_poss_score_prob']\n",
    "    )\n",
    "    \n",
    "    # Compute the metrics as differences from adjusted previous probabilities\n",
    "    data['VAEP'] = (\n",
    "        (data['score_prob'] - data['adjusted_prev_score_prob']) -\n",
    "        (data['concede_prob'] - data['adjusted_prev_concede_prob'])\n",
    "    )\n",
    "    data['GPA'] = (\n",
    "        (data['poss_score_prob'] - data['adjusted_prev_poss_score_prob']) -\n",
    "        (data['poss_concede_prob'] - data['adjusted_prev_poss_concede_prob'])\n",
    "    )\n",
    "    data['WPA'] = data['win_prob'] - data['adjusted_prev_win_prob']\n",
    "    \n",
    "    # For the first action of each match (where previous values are NaN), use current probabilities\n",
    "    data['VAEP'] = data['VAEP'].fillna(data['score_prob'] - data['concede_prob'])\n",
    "    data['GPA'] = data['GPA'].fillna(data['poss_score_prob'] - data['poss_concede_prob'])\n",
    "    data['WPA'] = data['WPA'].fillna(data['win_prob'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Compute metrics for training and test data\n",
    "train_metrics = compute_metrics(train_data)\n",
    "test_metrics = compute_metrics(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiments with adding GPA as a feature\n",
    "\n",
    "# Model 6: WPA1\n",
    "# Set up reduced set of features\n",
    "modified_params = catboost_params.copy()\n",
    "modified_params.pop('cat_features')\n",
    "print(modified_params)\n",
    "# Train and evaluate model\n",
    "catboost_wpa1, acc_wpa1, auc_wpa1, brier_wpa1 = train_and_evaluate(\n",
    "    train_data[['GPA']], y_match_train,\n",
    "    test_data[['GPA']], y_match_test,\n",
    "    model_params=modified_params,\n",
    "    model_name=\"WPA1\",\n",
    ")\n",
    "\n",
    "# Model 7: WPA2\n",
    "# Prepare features\n",
    "X_train_gpa = X_train.copy()\n",
    "X_test_gpa = X_test.copy()\n",
    "X_train_gpa['GPA'] = train_metrics['GPA']\n",
    "X_test_gpa['GPA'] = test_metrics['GPA']\n",
    "numeric_cols = numeric_cols + ['GPA']\n",
    "# Get new feature names\n",
    "feature_names_gpa = X_train_gpa.columns\n",
    "# Train and evaluate model\n",
    "catboost_wpa2, acc_wpa2, auc_wpa2, brier_wpa2 = train_and_evaluate(\n",
    "    X_train_gpa, y_match_train,\n",
    "    X_test_gpa, y_match_test,\n",
    "    model_params=catboost_params,\n",
    "    model_name=\"WPA2\",\n",
    "    feature_names=feature_names_gpa\n",
    ")\n",
    "\n",
    "# Use the above models to compute WPA1 and WPA2\n",
    "metric_dict = {'WPA1': [catboost_wpa1, train_data[['GPA']], test_data[['GPA']]], 'WPA2': [catboost_wpa2,X_train_gpa,X_test_gpa]}\n",
    "def compute_wpa_new(data, model, metric, features):\n",
    "    \"\"\"\n",
    "    Compute alternative WPA metrics\n",
    "    \"\"\"\n",
    "    # Compute raw probabilities for the current action\n",
    "    data[f'{metric}_raw'] = model.predict_proba(features)[:, 1]\n",
    "    # Shift raw probabilities to get the previous action's value within each match\n",
    "    data[f'prev_{metric}_raw'] = data.groupby('match_id')[f'{metric}_raw'].shift()\n",
    "    data[f'adjusted_prev_{metric}'] = np.where(\n",
    "        data['team_change'] == 0,\n",
    "        data[f'prev_{metric}_raw'],\n",
    "        1 - data[f'prev_{metric}_raw']\n",
    "    )\n",
    "    # Compute the metric as change in win probability, filling NaN with raw value\n",
    "    metric_values = (data[f'{metric}_raw'] - data[f'adjusted_prev_{metric}']).fillna(data[f'{metric}_raw'])\n",
    "    \n",
    "    return metric_values\n",
    "\n",
    "# Compute on training and test sets\n",
    "for metric, (model, train_features, test_features) in metric_dict.items():\n",
    "    train_metrics[metric] = compute_wpa_new(train_data, model, metric, train_features)\n",
    "    test_metrics[metric] = compute_wpa_new(test_data, model, metric, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns for both datasets\n",
    "drop_cols = [col for col in train_metrics.columns if \"prev\" in col or \"scores_in\" in col or \"concedes_in\" in col]\n",
    "train_metrics = train_metrics.drop(columns=drop_cols)\n",
    "test_metrics = test_metrics.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save metrics\n",
    "# train_metrics.to_csv('data/train_metrics.csv', index=False)\n",
    "# test_metrics.to_csv('data/test_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CROSS VALIDATION CODE\n",
    "# # Example for match outcome model\n",
    "# X = X_train \n",
    "# y = y_match_train\n",
    "# # Define models and their parameter distributions\n",
    "# models = {\n",
    "#     'CatBoost': {\n",
    "#         'model': CatBoostClassifier(random_state=42, verbose=1, cat_features=categorical_cols, auto_class_weights='Balanced'),\n",
    "#         'params': {\n",
    "#             'iterations': randint(100, 400),\n",
    "#             'depth': randint(4, 10),\n",
    "#             'l2_leaf_reg': randint(2, 10),\n",
    "#             'random_strength': uniform(0, 10),\n",
    "#             # 'border_count': randint(32, 255),\n",
    "#         },\n",
    "#         'X': X  # Original features\n",
    "#     },\n",
    "# }\n",
    "# results, comparison, X_dict = run_model_selection(models=models, scoring='neg_brier_score', X=X, y=y, n_iter=3, n_jobs=-1, cv_strategy=cv_strategy) \n",
    "# eval_best_model(results, comparison, 'neg_brier_score', X_dict, y, cv_strategy=cv_strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
